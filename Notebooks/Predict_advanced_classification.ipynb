{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd47da44",
      "metadata": {
        "id": "cd47da44"
      },
      "source": [
        "### Importation of the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf61f3c",
      "metadata": {
        "id": "bdf61f3c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de4bfc8",
      "metadata": {
        "id": "9de4bfc8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "#Data Analysis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "#Data Preprocessing and Feature Engineering\n",
        "from textblob import TextBlob\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VWfCpk5Lv_R",
        "outputId": "d585cd43-cc79-4c82-b945-9617619829bc"
      },
      "id": "9VWfCpk5Lv_R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4718f356",
      "metadata": {
        "id": "4718f356"
      },
      "source": [
        "### Importation and initial exploration of the data before cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f142c427",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "f142c427",
        "outputId": "c16dec96-0f31-46ab-c1b6-2fa542dd4822"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5221ed84876a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ],
      "source": [
        "# Import the data set\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d3e9d9",
      "metadata": {
        "id": "e0d3e9d9"
      },
      "outputs": [],
      "source": [
        "# Observe the structure\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3614d06b",
      "metadata": {
        "id": "3614d06b"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sentiment.unique() # checking the unique sentiments present"
      ],
      "metadata": {
        "id": "rEI8XdkgX6CC"
      },
      "id": "rEI8XdkgX6CC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tweetid.nunique() # are the tweets from different people"
      ],
      "metadata": {
        "id": "Ka91_fKgYG0D"
      },
      "id": "Ka91_fKgYG0D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1950881",
      "metadata": {
        "id": "d1950881"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4643ab6",
      "metadata": {
        "id": "c4643ab6"
      },
      "outputs": [],
      "source": [
        "df.message.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0ef98c",
      "metadata": {
        "id": "bd0ef98c"
      },
      "outputs": [],
      "source": [
        "# Getting the details of some of the messages in the first 10 messages\n",
        "for i in range(1,11):\n",
        "    \n",
        "    print('message number ',i)\n",
        "    print('===================================================================================')\n",
        "    print(df.message[i])\n",
        "    print(\" \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YtM53nFbi9bt"
      },
      "id": "YtM53nFbi9bt"
    },
    {
      "cell_type": "markdown",
      "id": "48c334d2",
      "metadata": {
        "id": "48c334d2"
      },
      "source": [
        "### From the above we can see that some cleaning needs to be done \n",
        "1. Urls\n",
        "2. Hashtags\n",
        "3. stopwords\n",
        "4. punctuations\n",
        "5. words with @"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b53aa0",
      "metadata": {
        "id": "95b53aa0"
      },
      "source": [
        "#### suggestion We can create a function to deal with the cleaning of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1451845b",
      "metadata": {
        "id": "1451845b"
      },
      "source": [
        "### Text Cleaning\n",
        "* Removing Noise\n",
        "* Tokenisation\n",
        "* Stemming\n",
        "* Lemmatisation\n",
        "* Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4caafa41",
      "metadata": {
        "id": "4caafa41"
      },
      "outputs": [],
      "source": [
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us see how many sentiments are present for each time of sentiment\n",
        "df.sentiment.value_counts().plot(kind = 'bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lwDKf71NXMpI"
      },
      "id": "lwDKf71NXMpI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining useful functions for cleanig of the data"
      ],
      "metadata": {
        "id": "JlJw5Ro8wLJm"
      },
      "id": "JlJw5Ro8wLJm"
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "print(list(string.punctuation))"
      ],
      "metadata": {
        "id": "UTWUKiOOoBzI"
      },
      "id": "UTWUKiOOoBzI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
        "tokeniser = TreebankWordTokenizer()\n"
      ],
      "metadata": {
        "id": "i0vSxLTHliY2"
      },
      "id": "i0vSxLTHliY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "mqvmkVcMnMEz"
      },
      "id": "mqvmkVcMnMEz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "hjPKtrj2nY7B"
      },
      "id": "hjPKtrj2nY7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text, flags=re.MULTILINE)\n",
        "    return text"
      ],
      "metadata": {
        "id": "sefHPVOtY2yE"
      },
      "id": "sefHPVOtY2yE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def form_sentence(tweet):\n",
        "    tweet_blob = TextBlob(tweet)\n",
        "    return ' '.join(tweet_blob.words)"
      ],
      "metadata": {
        "id": "wDH5UADVfgl2"
      },
      "id": "wDH5UADVfgl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_user_alpha(tweet):\n",
        "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
        "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
        "    clean_s = ' '.join(clean_tokens)\n",
        "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
        "    return clean_mess"
      ],
      "metadata": {
        "id": "sUhNc9MgfzEd"
      },
      "id": "sUhNc9MgfzEd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def normalization(tweet_list):\n",
        "#  lem = WordNetLemmatizer()\n",
        " # normalized_tweet = []\n",
        " # for word in tweet_list:\n",
        " # normalized_text = lem.lemmatize(word,'v')\n",
        "  #normalized_tweet.append(normalized_text)\n",
        " # return normalized_tweet"
      ],
      "metadata": {
        "id": "-PrAQt8egUjM"
      },
      "id": "-PrAQt8egUjM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "oHhm_vhRmRyc"
      },
      "id": "oHhm_vhRmRyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(post):\n",
        "    pure_words= []\n",
        "    clean_sentence = ''.join([l for l in post if l not in list(string.punctuation)]).split()\n",
        "    for word in clean_sentence:\n",
        "      word =\"\".join(c for c in word if (c.isalpha() or c==\" \"))\n",
        "    \n",
        "      pure_words.append(word)\n",
        "\n",
        "\n",
        "    return ' '.join([str(item) for item in pure_words])"
      ],
      "metadata": {
        "id": "1OfZeHTTgh17"
      },
      "id": "1OfZeHTTgh17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AsMAYD2mxAHh"
      },
      "id": "AsMAYD2mxAHh"
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english') # This function will be used to create stems\n",
        "def mbti_stemmer(words, stemmer):\n",
        "    return [stemmer.stem(word) for word in words]"
      ],
      "metadata": {
        "id": "BHZDGPgPmeGq"
      },
      "id": "BHZDGPgPmeGq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mbti_lemma(words, lemmatizer): # This function will be used to lemmatize\n",
        "    return [lemmatizer.lemmatize(word) for word in words] "
      ],
      "metadata": {
        "id": "TPYKXQmtnerR"
      },
      "id": "TPYKXQmtnerR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing words starting with @ and also avoiding combination of words\n",
        "def clean_text(X):\n",
        "\n",
        "  X = X.split()\n",
        "  X_new = [x for x in X if not x.startswith(\"@\")] # removing words starting with @\n",
        "  X_new = [x for x in X_new if len(x) < 9]        # removing words longer than 8\n",
        "  X_new = [x for x in X_new if x != 'rt']         # removing str rt from text, it appears too many times\n",
        "\n",
        "  return ' '.join(X_new)"
      ],
      "metadata": {
        "id": "5_vEt2ikrkBi"
      },
      "id": "5_vEt2ikrkBi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrangle(df,post_column):\n",
        "  \"\"\" the function takes in two arguments a dataframe and a name column\n",
        "  for cleaning. It create three new columns a)tokens  b)stem c) lemma\n",
        "  It then returns the wrangled dataframe. This function will be used\n",
        "  on both the train and test data to clean and preprocess both \"\"\"\n",
        "  # remove punctuations\n",
        "  df[post_column] = df[post_column].apply(remove_punctuation)\n",
        "\n",
        "  #lowercase\n",
        "  df[post_column] = df[post_column].str.lower()\n",
        "\n",
        "  # remove urls\n",
        "  df[post_column] = df[post_column].apply(remove_urls)\n",
        "\n",
        "  # remove words starting with @ from messages\n",
        "  df[post_column] = df[post_column].apply(clean_text)\n",
        "  \n",
        "  # tokenize\n",
        "  df['tokens'] = df[post_column].apply(tokeniser.tokenize)\n",
        "\n",
        "  #  stematize\n",
        "  df['stem'] = df['tokens'].apply(mbti_stemmer, args=(stemmer, ))\n",
        "\n",
        "  # lemmatize\n",
        "  df['lemma'] = df['tokens'].apply(mbti_lemma, args=(lemmatizer, ))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "7A-uBlvNj6Vs"
      },
      "id": "7A-uBlvNj6Vs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrangle(df,'message')"
      ],
      "metadata": {
        "id": "tb2krAeGpNTB"
      },
      "id": "tb2krAeGpNTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[9]"
      ],
      "metadata": {
        "id": "4Td_NZNWp-6X"
      },
      "id": "4Td_NZNWp-6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lets view the first ten lines again after cleaning"
      ],
      "metadata": {
        "id": "smA8JoEdjAR1"
      },
      "id": "smA8JoEdjAR1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the details of some of the messages in the first 10 messages\n",
        "for i in range(1,11):\n",
        "    \n",
        "    print('message number ',i)\n",
        "    print('===================================================================================')\n",
        "    print(df.message[i])\n",
        "    print(\" \")\n"
      ],
      "metadata": {
        "id": "sTD412vdi_YV"
      },
      "id": "sTD412vdi_YV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y3ts652mqmSW"
      },
      "id": "Y3ts652mqmSW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "iRQhcE7AhZ5q"
      },
      "id": "iRQhcE7AhZ5q"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b4FU1L9_he8D"
      },
      "id": "b4FU1L9_he8D",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Predict advanced classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}